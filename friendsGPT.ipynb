{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd60b652-e919-4129-afcd-126718f48b3a",
   "metadata": {},
   "source": [
    "# Model Overall Structure\n",
    "\n",
    "1. **Token + Positional Embedding**  \n",
    "   Each character in the input is embedded into a vector based on:\n",
    "   - Its identity (**token embedding**)\n",
    "   - Its position within the block (**positional embedding**)  \n",
    "   These are summed to form the initial input vectors for each token.\n",
    "\n",
    "2. **Stack of Transformer Blocks**  \n",
    "   The model consists of multiple transformer blocks (`n_layer` determines how many). Each block follows this pattern:\n",
    "   - `LayerNorm → Self-Attention → Residual connection`\n",
    "   - `LayerNorm → Feed Forward → Residual connection`\n",
    "\n",
    "3. **Self-Attention Mechanism**  \n",
    "   - Each token produces a **query**, **key**, and **value** vector.\n",
    "   - Attention weights are computed as `softmax(QKᵀ / √d)`, masked to prevent attending to future tokens (causal attention).\n",
    "   - The result is a weighted combination of values, capturing contextual information.\n",
    "   - Multi-head attention is used to allow the model to attend to different representation subspaces.\n",
    "\n",
    "4. **Feed Forward Network (FFN)**  \n",
    "   - A two-layer MLP: `Linear(n_emb → 4×n_emb) → ReLU → Linear(4×n_emb → n_emb)`\n",
    "   - This introduces non-linearity and enables more complex transformations for each token independently.\n",
    "\n",
    "5. **Final Projection**  \n",
    "   - After all transformer blocks, a final `LayerNorm` is applied.\n",
    "   - The output is passed through a linear layer projecting from `n_emb` to `vocab_size`, yielding **unnormalized logits** for the next token.\n",
    "\n",
    "6. **Text Generation**  \n",
    "   - A softmax converts logits into probabilities.\n",
    "   - The next token is sampled from this distribution and appended to the sequence.\n",
    "   - This process is repeated for generating sequences of desired length.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec1c1a8-7858-4b6d-b0a8-ea909da5914b",
   "metadata": {},
   "source": [
    "## Imports and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc28e908-9feb-43fc-88c0-de64b5d300f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cbf3451-989e-40cc-9163-694ee58d73bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256\n",
    "batch_size = 64\n",
    "max_iters = 15000            # ~60 epochs\n",
    "eval_interval = 750          # Evaluate every ~5 epochs\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 100\n",
    "n_emb = 384\n",
    "n_layer = 6\n",
    "n_head = 6\n",
    "dropout = 0.2\n",
    "temperature = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b79010-d2b1-40b0-8bf5-3e6c55110679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1eed1329730>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(2408)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc49f41e-f9d2-490f-86da-c504b972bcde",
   "metadata": {},
   "source": [
    "## Text Processing\n",
    "Text is read from a .txt file.\n",
    "Chars found in the file is mapped to an integer for encoding nad decoding purposes.\n",
    "90% of the dataset is assigned to the train set and the remaining 10% validation set.\n",
    "get_batch is defined, which splits the data into tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "391932eb-710f-46fe-a294-18aa78f95e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('combined.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "243f2caa-31e5-4d33-b142-bd0a227085bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0e15faf-5aed-49d6-a5c9-03233fcfea3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a map for char to int and vv\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] #encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) #decoder: take a list of integers, output a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf47266f-892b-4581-806f-4a1fd5a344c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = ByteLevelBPETokenizer()\n",
    "# tokenizer.train(files=[\"combined.txt\"], vocab_size=50257, min_frequency=2, special_tokens=[\n",
    "#     \"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9427753c-65a0-4d39-9549-1419c1a5aa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.save_model(\".\", \"bpe_tokenizer\")\n",
    "# tokenizer = LoadedTokenizer(\"bpe_tokenizer-vocab.json\", \"bpe_tokenizer-merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b521c0a6-7f57-4aa0-9e99-c9ad6757ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded = tokenizer.encode(text)\n",
    "# data = torch.tensor(encoded.ids, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbea3372-7522-418a-a7ad-f489996498cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data))\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f791a67d-e9fd-406e-91bb-63f7e274462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.tensor(encoded.ids, dtype=torch.long)\n",
    "# n = int(0.9*len(data))\n",
    "# train_data = data[:n]\n",
    "# val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1c6df771-51e4-47fe-8534-16da9004c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get a batch of data\n",
    "def get_batch(split):\n",
    "    #generates a small batch of input and target\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) #random offsets\n",
    "    \n",
    "    #get a 4x8 tensor\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]) \n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)  #move to device\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed449278-2e7a-4559-8e27-812b408ec594",
   "metadata": {},
   "source": [
    "## Loss Estimator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2a155f9-bfbb-4d31-b11a-569348aec357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to estimate loss\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()  #set the model to evaluation mode\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            x, y = get_batch(split)\n",
    "            # x, y = x.to(device), y.to(device)\n",
    "            logits, loss = model(x, y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()  #set the model back to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843e79f3-74ef-4e1e-9286-93cdcdaec65e",
   "metadata": {},
   "source": [
    "## Self-Attention Heads\n",
    "Each Head computes the key, query from each input. From this, the attention weight is calculated and the weight is used to compute the value vector.\n",
    "It also uses a triangular vector to make sure that it will only be affected by past and not by future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3283160-22a7-4134-85d4-93b2984da1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_emb, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)  # (B, T, head_size)\n",
    "        q = self.query(x)  # (B, T, head_size)\n",
    "        # compute attention scores\n",
    "        wei = q @ k.transpose(-2, -1) * C ** -0.5  # (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # apply causal mask\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)  # apply dropout to attention weights\n",
    "        v = self.value(x)  # (B, T, C)\n",
    "        out = wei @ v  # (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ff19de-70ae-4ac4-9891-62c09cd2a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_emb, n_emb)  # projection to the original embedding size\n",
    "        self.dropout = nn.Dropout(dropout)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)  # concatenate the outputs of all heads   \n",
    "        out = self.proj(out)  # project back to the original embedding size\n",
    "        out = self.dropout(out)\n",
    "        return out  # return the concatenated output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc24eb7-3bab-40af-92e4-480ffa845a9d",
   "metadata": {},
   "source": [
    "## Feed Forward Layer\n",
    "Adds non-linearity to each token using ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bfcaa77-d90e-4198-a7cb-c7d5229d1a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_emb):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_emb, 4 * n_emb),\n",
    "            nn.GELU(), \n",
    "            nn.Linear(4 * n_emb, n_emb),\n",
    "            nn.Dropout(dropout)  # dropout for regularization    \n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)  # apply the feedforward network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e71e20-2d6b-4f4f-bbab-1b42fbd6fc48",
   "metadata": {},
   "source": [
    "## Block\n",
    "Normalizes each token before self-attention. Each token is then normalized again before going through feed-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79067ab4-3e7e-4064-80a7-e755b3e35101",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_emb, n_head):\n",
    "        super().__init__()\n",
    "        heead_size = n_emb // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, heead_size)  # self-attention\n",
    "        self.ffwd = FeedForward(n_emb)  # feedforward network\n",
    "        self.ln1 = nn.LayerNorm(n_emb) \n",
    "        self.ln2 = nn.LayerNorm(n_emb)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))  # apply self-attention\n",
    "        x = x + self.ffwd(self.ln2(x))  # apply feedforward network\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2702aecb-0d6a-4cc8-b36d-77adf70dfbdf",
   "metadata": {},
   "source": [
    "## Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7094d7-a55f-4b2b-98f3-f74eb7eca2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)\n",
    "        self.pos_embedding_table = nn.Embedding(block_size, n_emb)  # positional embeddings\n",
    "        self.blocks = nn.Sequential(*[Block(n_emb, n_head=n_head) for _ in range(n_layer)])  # stack of transformer blocks\n",
    "        self.ln_f = nn.LayerNorm(n_emb)  \n",
    "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape  # B is batch size, T is block size\n",
    "\n",
    "        #idx and targets are both (B,T) tensor of integers\n",
    "        tkn_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.pos_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tkn_emb + pos_emb  \n",
    "        x = self.blocks(x)  # pass through the transformer blocks\n",
    "        x = self.ln_f(x)  # apply layer normalization\n",
    "        \n",
    "        logits = self.lm_head(x) # (B,T,vc size)\n",
    "        #logits are the unnormalized log probabilities of the next token\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets) #quality of logits based on targets\n",
    "\n",
    "        return logits, loss\n",
    "        \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        #idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            #cut the context to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]  # (B, T) get the last block_size tokens\n",
    "            #get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            #focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            #apply softmax to get probabilities\n",
    "            probs = F.softmax(logits / temperature, dim=-1)\n",
    "            #sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            #append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882caef-277d-4239-9bb2-8304473d817d",
   "metadata": {},
   "source": [
    "## Model Initialization / Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dc9d255-34b0-4d0b-8c91-c3954d7babce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the model    \n",
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8798286-45d7-423c-86a1-e13739dfcde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BigramLanguageModel()\n",
    "# model.load_state_dict(torch.load('model.pt'))\n",
    "# m = model.to(device)\n",
    "# optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1614e9-cf56-4ec8-ac4a-f0ac5aae5e9d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "90a55067-b77c-4a58-a04f-eee99eb5e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_loss = float('inf')  # Track the best validation loss\n",
    "checkpoint_path = \"checkpoint.pt\"  # File to save model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c49c446-5a9c-4217-96c2-a370ac7596e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Train loss 4.7751, Val loss 4.7732\n",
      "New best val loss! Model saved at iteration 0.\n",
      "Step 750: Train loss 1.4852, Val loss 1.5263\n",
      "New best val loss! Model saved at iteration 750.\n",
      "Step 1500: Train loss 1.2464, Val loss 1.3067\n",
      "New best val loss! Model saved at iteration 1500.\n",
      "Step 2250: Train loss 1.1527, Val loss 1.2196\n",
      "New best val loss! Model saved at iteration 2250.\n",
      "Step 3000: Train loss 1.0899, Val loss 1.1713\n",
      "New best val loss! Model saved at iteration 3000.\n",
      "Step 3750: Train loss 1.0483, Val loss 1.1413\n",
      "New best val loss! Model saved at iteration 3750.\n",
      "Step 4500: Train loss 1.0138, Val loss 1.1131\n",
      "New best val loss! Model saved at iteration 4500.\n",
      "Step 5250: Train loss 0.9872, Val loss 1.1007\n",
      "New best val loss! Model saved at iteration 5250.\n",
      "Step 6000: Train loss 0.9644, Val loss 1.0880\n",
      "New best val loss! Model saved at iteration 6000.\n",
      "Step 6750: Train loss 0.9433, Val loss 1.0792\n",
      "New best val loss! Model saved at iteration 6750.\n",
      "Step 7500: Train loss 0.9310, Val loss 1.0775\n",
      "New best val loss! Model saved at iteration 7500.\n",
      "Step 8250: Train loss 0.9101, Val loss 1.0757\n",
      "New best val loss! Model saved at iteration 8250.\n",
      "Step 9000: Train loss 0.8990, Val loss 1.0757\n",
      "New best val loss! Model saved at iteration 9000.\n",
      "Step 9750: Train loss 0.8868, Val loss 1.0726\n",
      "New best val loss! Model saved at iteration 9750.\n",
      "Step 10500: Train loss 0.8738, Val loss 1.0694\n",
      "New best val loss! Model saved at iteration 10500.\n",
      "Step 11250: Train loss 0.8613, Val loss 1.0723\n",
      "Step 12000: Train loss 0.8513, Val loss 1.0715\n",
      "Step 12750: Train loss 0.8351, Val loss 1.0739\n",
      "Step 13500: Train loss 0.8249, Val loss 1.0797\n",
      "Step 14250: Train loss 0.8156, Val loss 1.0777\n"
     ]
    }
   ],
   "source": [
    "for iter in range(max_iters):\n",
    "    \n",
    "    #evaluate the model every eval_interval iterations\n",
    "    if iter % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"Step {iter}: Train loss {losses['train']:.4f}, Val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"New best val loss! Model saved at iteration {iter}.\")\n",
    "\n",
    "    #get a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "    #evaluate the model\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    #backpropagation\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5898026-cae7-4da2-b119-a9d3a8bf6e18",
   "metadata": {},
   "source": [
    "## Saving model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a06ec19-8848-4568-a36f-a878afded337",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(m.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586bf54e-2d25-4c9f-ad4e-8e83b9db3caa",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e4a68a9-6809-4732-bf91-1c959a4468b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ONE WITH THE OLD TO BUT O OD MOON!!!\n",
      "\n",
      "MONICA: Yeah.\n",
      "\n",
      "RACHEL: Yeah it's okay to so be okay, he's okay, this is not funny... , he's okay, got a call for a sec; it has no final.\n",
      "\n",
      "MONICA: Yes, because, he's got a big deal, and he's got a loong ring, and I need a way to hell with you.\n",
      "\n",
      "RACHEL: Why does a little fan of life.\n",
      "\n",
      "MONICA: Could you look it, you know when we were out to knee your name?\n",
      "\n",
      "RACHEL: Yeah, actually it was really huge. I was a name.\n",
      "\n",
      "MONICA: Yeah, we'd just leave them a noted a deal.\n",
      "\n",
      "JOEY: Ok, man, I went you out with her.\n",
      "\n",
      "MONICA: With her question and the classics shake.\n",
      "\n",
      "ROSS: No.\n",
      "\n",
      "CHANDLER: Wow, you don't make turned out any night.\n",
      "\n",
      "ROSS: Hey look. Ok.\n",
      "\n",
      "PHOEBE: So what do you guys want to do that? Why won't the good part?\n",
      "\n",
      "MONICA: It's not so obvious. Rachel, you're supposed to be a men. They don't like it been a little Turty back of Ben. They look all nice.\n",
      "\n",
      "ROSS: I'm sorry that you put your head.\n",
      "\n",
      "RACHEL: How do you do not think that?\n",
      "\n",
      "ROSS: Because you got me, it's my girlfriend, but you know, they got me one of you to just like that.\n",
      "\n",
      "RACHEL: Would you just grab me a coupo.\n",
      "\n",
      "ROSS: What'd you have a milk in the cheese?\n",
      "\n",
      "RACHEL: Sometimes make out labor that long. What does they see?\n",
      "\n",
      "ROSS: He have her head, oh. All right, so umm, we're havin' a loner in the cabinet so you but then there's no rese in it.\n",
      "\n",
      "RACHEL: Would you look at Jamasmia & I have a problem with your none?\n",
      "\n",
      "ROSS: Oh, I'd love to.\n",
      "\n",
      "RYAN: Yeah, me neither.\n",
      "\n",
      "ROSS: I have a problem skin.\n",
      "\n",
      "RACHEL: Well, just sometimes it's just because me.\n",
      "\n",
      "ROSS: Oh, my plocked.\n",
      "\n",
      "RACHEL: Hey, that was a good idea.\n",
      "\n",
      "ROSS: Oh, she is my production.\n",
      "\n",
      "RACHEL: Yeah, \"well I didn't invite you.\"\n",
      "\n",
      "ROSS: Uh uh Russ.\n",
      "\n",
      "RACHEL: I don't think I can decide.\n",
      "\n",
      "ROSS: Great. Wait, if it sounds well.\n",
      "RACHEL: Well, I uh, you know it's what it's not what it's like I'm talking about.\n",
      "\n",
      "ROSS: Rachel, I'm sorry, I just guess.\n",
      "\n",
      "RACHEL: And I forgot you lookin' out on your own edgent rose treats.\n",
      "\n",
      "ROSS: Oh God, oh, I know, I know, but uh, I'm sorry I'm just so in it.\n",
      "\n",
      "RACHEL:  Hey Ross, did you get your assistant working out on your third new Val Mupber one?\n",
      "\n",
      "ROSS: Well I, I had a vulchamabe.\n",
      "\n",
      "RACHEL: Doctor Ross, I'm done good. \n",
      "\n",
      "ROSS: Dad, the Rurse always gettingoing out of here, How could you not tell her what him is.\n",
      "\n",
      "RACHEL: Thank God. So, Tilly he act like him.\n",
      "\n",
      "PHOEBE: Oh, yeah.\n",
      "\n",
      "ROSS: That's what I would do.\n",
      "\n",
      "RACHEL: Thank you.\n",
      "\n",
      "ROSS: Ok, uh, we're so impressing and leave the table and the job. \n",
      "\n",
      "RACHEL: Oh, just so he can't help you hear this. \n",
      "\n",
      "ROSS: Hey look what I got. Oh, I don't know what to think you look uncomfortable.\n",
      "\n",
      "MONICA: Ross, I'm not suranged that right now. \n",
      "\n",
      "ROSS: I know.\n",
      "Ross: Oh, I guess it is. I just don't dear when you know, don't worry.\n",
      "\n",
      "MONICA:  I mean, I love you so much.\n",
      "\n",
      "ROSS: I'm, uh... I'm not Mon.\n",
      "\n",
      "MONICA: Well, that's uh... oh c'mon. I mean, uh... I went out and it's the guy who had in a bee too.\n",
      "\n",
      "ROSS: So Monica, what? I think I'm makin' on in the new Ross's bee, appart of our lines.\n",
      "\n",
      "MONICA: Was that our sale beer?\n",
      "\n",
      "ROSS: Uh, how dirty has you said it was a good actor, and, all of the turtlent rent centure on the face.\n",
      "\n",
      "MONICA: No way, no way.\n",
      "\n",
      "ROSS: Yeah.\n",
      "\n",
      "MONICA: What's going on here now?\n",
      "\n",
      "ROSS: But look at me and call white this or a little of his hearts.\n",
      "\n",
      "MONICA: Well, that's nice to propose.\n",
      "\n",
      "MONICA: No, I know it's about to get it off my stuff.\n",
      "\n",
      "ROSS:  Well, did you get your shot at 4:00?\n",
      "\n",
      "MONICA: Yeah.\n",
      "\n",
      "MONICA: I guess you know, I better tell my eggg almost friends about me, but you think it's ok and I think I don't realize that they should call you Irone in the staes.\n",
      "\n",
      "ROSS: All right. What? Wat a memberyos. Watch.\n",
      "\n",
      "MONICA: Oh my God.\n",
      "\n",
      "PHOEBE: Yeah, what?\n",
      "\n",
      "RACHEL:  I'm not gonna tell you, he's just gonna flyer again.\n",
      "\n",
      "MONICA: Ross, we'll see you outside than the record and outside me together.\n",
      "\n",
      "ROSS: Ok. Make.\n",
      "\n",
      "MONICA: Look at that, get the rules!\n",
      "\n",
      "RACHEL:  Uhh, you know what, we're gonna be on our friends and stuff.\n",
      "\n",
      "MONICA: Alright, her chance. What is the most amazy? Why, he knows what we got in once because we could leave us a little guy that lampoose telemens.\n",
      "\n",
      "RACHEL: Oh, I know. We're so worried about this addic on your thing.\n",
      "\n",
      "MONICA: Excellent. Hi. I love you.\n",
      "\n",
      "RACHEL: OK, OK.  How about this time today, the thing is just worth coming.\n",
      "\n",
      "GRANDMOTHER: Hi guys.\n",
      "ROSS: Hey Chandler .\n",
      "\n",
      "CHANDLER: Hey.\n",
      "\n",
      "ROSS: Ooh, oh.\n",
      "\n",
      "CHANDLER: What's this guy name Ben?\n",
      "\n",
      "RACHEL: Well, I would have to talk. Well, I don't know my daughter then it's working nice to happen.\n",
      "\n",
      "CHANDLER: Wow, come on. So if you know what would be a friend and would like to kiss you again, does anybody know, you know?\n",
      "\n",
      "GIRL BULLY: How can I say it.\n",
      "\n",
      "MONICA: What about me?\n",
      "\n",
      "JOEY: I think that too.\n",
      "CHANDLER: Well, what's this is Tity.\n",
      "\n",
      "MONICA: You know?\n",
      "\n",
      "CHANDLER: OK.... take the point. \n",
      "\n",
      "MONICA: Alright, turn 'em out.\n",
      "\n",
      "ROSS: I think it's my secludition.\n",
      "\n",
      "MONICA:  Hello.\n",
      "\n",
      "ROSS: Hello.\n",
      "\n",
      "MONICA: Hello. Hello sweetie, how, how are you nerving to ... instead of all, not Susan was that now, so we're cone to the recipe. And we both thank you fourth me. And then, who told you a crumbing with me.\n",
      "\n",
      "PHOEBE: Yeah.\n",
      "\n",
      "CHANDLER: Yeah. Do I leave him the big painfully his painfular block?\n",
      "\n",
      "MRS. GELLER: I couldn't be my third for ya.\n",
      "\n",
      "CHANDLER: You're welcome. The many name is Janice. So, how's my life?\n",
      "\n",
      "MONICA: I hope it's Richard's head. It's Richard's always been home and coming up.\n",
      "\n",
      "MR. GELLER: Of course it is. It takes feel like I could be heading on the scene if it wasn't about the delivery love.\n",
      "\n",
      "ALL: Fine. Good love you. \n",
      "\n",
      "PHOEBE: Hey, I'm married with you. Oh by the way. Bye Hannice Mark.\n",
      "\n",
      "MR A: Bye mome, you're welcomed to think about fast.\n",
      "\n",
      "PHOEBE: Yeah, we were just being so name.\n",
      "\n",
      "MRS A: I think they're just looking at it myself.\n",
      "\n",
      "PHOEBE: Oh by the waitress, they're right.\n",
      "\n",
      "MRS GREEN: I am, it's kinda great. Oh I know, it's kinda sure you've had another money, but it's like I'm really naked. I want you to be with mamazing, you know, ok. And now wa-- wait, look-look, I have this because I have to uh, oh, a Carper funeral man.\n",
      "\n",
      "ROSS: Woah, you're not allowed.\n",
      "\n",
      "MONICA: Oh, this is it! I can picture the weekend of my life. Then I guess we can take a picture of your uh, burning in the world and run into the lamp.\n",
      "\n",
      "ROSS: Yeah.\n",
      "\n",
      "MONICA: Goodnight.\n",
      "\n",
      "ROSS: Yes.\n",
      "\n",
      "MONICA: I don't know.\n",
      "\n",
      "ROSS: You know, I don't know.\n",
      "\n",
      "CHANDLER: Listen I borrowed my own blood, but I should, I have heard of seeing you from who had rooms together.\n",
      "\n",
      "ROSS: Look but I'm not so looking at you, but I'm not a horrible man.\n",
      "\n",
      "PHOEBE: You're an allergy man. You're a kad. Here's this pirty morning.\n",
      "\n",
      "ROSS: Well... I think you know sometimes about her.\n",
      "\n",
      "PHOEBE: Ooh.\n",
      "\n",
      "PHOEBE: Ok, good. \n",
      "\n",
      "CHANDLER: Really?\n",
      "\n",
      "PHOEBE: Yeah, yeah. I can answer that her of sometimes could be the first chain answering questions.\n",
      "\n",
      "ROSS: Hey Pheebs, what's goin' on your head? I'm Grandbaggin to the lady!\n",
      "\n",
      "PHOEBE: Oh, God, I hope there were what happened between these for the seconds, so where is everybody gonna be a friend?\n",
      "\n",
      "ROSS: Well, I think we're gonna tell each other, he's smoothing that she's gonna tell her I told Chandler.\n",
      "\n",
      "PHOEBE: Hello.\n",
      "\n",
      "ROSS: Hey uh, alright. No, it's Phoebe. How was your favorite one? He was a ball...\n",
      "\n",
      "PHOEBE: Oh OK, OK, so that's the brilliance was just so that he plays straight the setting at Gentleman.\n",
      "\n",
      "RACHEL: Oh, thank you guys.\n",
      "\n",
      "PHOEBE: That's the cab, what do you say?\n",
      "\n",
      "JOEY: I don't know, my car is because she knows when there was to compliment, do you remember, you were really obstages about my annulment?\n",
      "\n",
      "RACHEL: What? Don't you want me to talk to me what you're gonna do? I want you to know about me and you want yourself. I want your memories and she didn't want it.\n",
      "\n",
      "MONICA: Alright. \n",
      "\n",
      "RACHEL: Oh God, you know what? You don't want it. We want to do this for moment.\n",
      "\n",
      "JOEY: Faster what you're doing there are we now.\n",
      "\n",
      "PHOEBE: Oh.\n",
      "\n",
      "RACHEL: I know. We're walking away for our kids.\n",
      "\n",
      "PHOEBE: You kids like your Stephan numbers.\n",
      "\n",
      "RACHEL: Ok, what does Joey need a kid last night?\n",
      "\n",
      "JOEY: Well yea.\n",
      "KIDS: Goodbye JOEY! Well, I know mine. Dear. Everything's gonna be ok. You're ok, you're ok?\n",
      "\n",
      "JOEY: Uhh, you're one of my still lost of the shower.\n",
      "PHOEBE: Oh my god, you know what the most happened last night?\n",
      "\n",
      "JOEY: What?\n",
      "\n",
      "CHANDLER: They're not a good idea.\n",
      "\n",
      "JOEY: Well, what do ya smells like sitting?\n",
      "\n",
      "CHANDLER: The idea. I have to smell some releal. We'll see.\n",
      "\n",
      "JOEY:  Oh my God. I smell that one too.\n",
      "\n",
      "CHANDLER: What's in this? I don't live there would be no. Whatever you lived. Ok, I never wanted a whole ago at all.\n",
      "\n",
      "JOEY: Ok, ok, yeah, if I want to pay a dog home, it's like the dock.\n",
      "\n",
      "CHANDLER: Oh, no!\n",
      "\n",
      "JOEY: Hey, here you go. There's it is. Ok, don't say there's one that body has becances to anything with me, like you.  Ok, look, that's it. \n",
      "\n",
      "CHANDLER: With a car in Bob oven.\n",
      "\n",
      "JOEY: Night over. It's not that bad. What's it bad?\n",
      "\n",
      "CHANDLER: I said. . Josepare the girl ate all.\n",
      "\n",
      "JOEY: Oh, good. I love that place.\n",
      "\n",
      "CHANDLER: OK, now do I love you Ross' covered?\n",
      "\n",
      "JOEY: Oh yeah, it would've been throwing the guy you home, or you know, what I meant. I just did with the smells good one.\n",
      "\n",
      "CHANDLER: Well, actually when people live you are sure you need complimentation for three of us?\n",
      "\n",
      "MONICA: Uh, no, right now go look at the brighten ruine. The next blood is a deal.\n",
      "\n",
      "JOEY: Oh, well w-w-wait, wait, wait.\n",
      "\n",
      "MONICA: It's cool!\n",
      "\n",
      "JOEY:  FAAY!\n",
      "\n",
      "MONICA: What a big and of flatles.'\n",
      "\n",
      "CHANDLER: What's happening? I meant to live with somether.\n",
      "\n",
      "JOEY: Oh, ok.\n",
      "\n",
      "MONICA: Ok. I got you it. Is there anything I can do in or nothing enough with a wosse than I'm not ever gonna wanna get my mine man, you know, then I don't wanna go to work, for what would I do with your sister's relationship, I don't want to know...\n",
      "\n",
      "MONICA: Well if I have to know about my man was, I wanna get one of those alives.\n",
      "\n",
      "RACHEL: Ok, you sit there, you know, because I have to pee help with does it, you know? There is, can you call it the collar?\n",
      "\n",
      "JOEY: Yeah, ok, now I can coll\n"
     ]
    }
   ],
   "source": [
    "start = \"THE ONE WITH THE \"\n",
    "context = torch.tensor([encode(start)], dtype=torch.long, device=device)  # wrap in tensor and send to device\n",
    "generated = m.generate(context, max_new_tokens=10000)[0].tolist()\n",
    "print(decode(generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06e43066-6687-48c7-9f17-d97e433ff67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = \"THE ONE WITH THE \"\n",
    "# context = torch.tensor([tokenizer.encode(start)], dtype=torch.long, device=device)  # wrap in tensor and send to device\n",
    "# generated = m.generate(context, max_new_tokens=1000)[0].tolist()\n",
    "# print(tokenizer.decode(generated))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
